#!/bin/bash
# ============================================================================
# Setup Script
# ============================================================================
# Run this after `terraform apply` to automatically populate the Airflow
# .env file with Terraform outputs.
#
# Usage: ./scripts/setup_env.sh
# ============================================================================

set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_DIR="$(dirname "$SCRIPT_DIR")"
TERRAFORM_DIR="$PROJECT_DIR/terraform"
AIRFLOW_DIR="$PROJECT_DIR/airflow"
ENV_FILE="$AIRFLOW_DIR/.env"

echo "ðŸ”§ Generating Airflow .env from Terraform outputs..."
echo ""

# Check terraform is initialized
if [ ! -d "$TERRAFORM_DIR/.terraform" ]; then
    echo "âŒ Terraform not initialized. Run: cd terraform && terraform init"
    exit 1
fi

# Get outputs
cd "$TERRAFORM_DIR"
OUTPUTS=$(terraform output -json 2>/dev/null)

if [ -z "$OUTPUTS" ] || [ "$OUTPUTS" == "{}" ]; then
    echo "âŒ No Terraform outputs found. Run: cd terraform && terraform apply"
    exit 1
fi

# Parse outputs and write .env
cat > "$ENV_FILE" << EOF
# Auto-generated by scripts/setup_env.sh on $(date)
# Do not edit manually â€” re-run the script after terraform apply.

DATA_BUCKET=$(echo "$OUTPUTS" | python3 -c "import sys, json; print(json.load(sys.stdin)['data_bucket_name']['value'])")
SCRIPTS_BUCKET=$(echo "$OUTPUTS" | python3 -c "import sys, json; print(json.load(sys.stdin)['scripts_bucket_name']['value'])")
EMR_APP_ID=$(echo "$OUTPUTS" | python3 -c "import sys, json; print(json.load(sys.stdin)['emr_serverless_app_id']['value'])")
EMR_EXECUTION_ROLE_ARN=$(echo "$OUTPUTS" | python3 -c "import sys, json; print(json.load(sys.stdin)['emr_execution_role_arn']['value'])")
GLUE_DB_BRONZE=$(echo "$OUTPUTS" | python3 -c "import sys, json; print(json.load(sys.stdin)['glue_database_bronze']['value'])")
GLUE_DB_SILVER=$(echo "$OUTPUTS" | python3 -c "import sys, json; print(json.load(sys.stdin)['glue_database_silver']['value'])")
GLUE_DB_GOLD=$(echo "$OUTPUTS" | python3 -c "import sys, json; print(json.load(sys.stdin)['glue_database_gold']['value'])")
AWS_REGION=$(echo "$OUTPUTS" | python3 -c "import sys, json; print(json.load(sys.stdin)['aws_region']['value'])")
EOF

echo "âœ… Airflow .env file written to: $ENV_FILE"
echo ""
echo "Contents:"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€"
cat "$ENV_FILE"
echo ""
echo "Next steps:"
echo "  1. cd airflow && docker compose up -d"
echo "  2. Open http://localhost:8080 (admin/admin)"
echo "  3. Enable the 'nyc_taxi_monthly_pipeline' DAG"
